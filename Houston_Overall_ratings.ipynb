{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p58hYQ8tt88k",
        "outputId": "a2a64b4c-3af0-4aeb-b9a4-9cd8b4ef37ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-ae6375351e54>:5: DtypeWarning: Columns (46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  new_data = pd.read_csv(file_path)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/AirBnb Pre processed.csv'  # Replace with your file path\n",
        "new_data = pd.read_csv(file_path)\n",
        "\n",
        "# Example mapping (replace this with your actual mapping)\n",
        "zipcode_to_neighborhood = {\n",
        "    '02108': 'Beacon Hill',\n",
        "    '02109': 'North End',\n",
        "    # Add other mappings\n",
        "}\n",
        "\n",
        "# Function to fill missing neighborhood values\n",
        "def fill_neighborhood(row):\n",
        "    if pd.isna(row['Neighborhood']) and row['Zipcode'] in zipcode_to_neighborhood:\n",
        "        return zipcode_to_neighborhood[row['Zipcode']]\n",
        "    else:\n",
        "        return row['Neighborhood']\n",
        "\n",
        "# Apply the function to the DataFrame\n",
        "new_data['Neighborhood'] = new_data.apply(fill_neighborhood, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omQqWcTg4sR-",
        "outputId": "1fcab361-81db-4557-8ba9-65387b1384d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 164.9073720752416\n",
            "R-squared: 0.22185312001728374\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/AirBnb Pre processed.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Drop rows where specified columns have NaN values\n",
        "columns_to_check = ['rating_ave_pastYear', 'numReviews_pastYear', 'numCancel_pastYear',\n",
        "                    'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "                    'prev_rating_ave_pastYear', 'prev_numReviews_pastYear',\n",
        "                    'prev_numCancel_pastYear', 'prev_num_5_star_Rev_pastYear',\n",
        "                    'prev_prop_5_StarReviews_pastYear', 'Property Type']\n",
        "data = data.dropna(subset=columns_to_check)\n",
        "\n",
        "# Drop specified columns\n",
        "columns_to_drop = ['Nightly Rate_tractQuartile', 'prev_Nightly Rate_tractQuartile',\n",
        "                   'available_days_aveListedPrice_tractQuartile',\n",
        "                   'prev_available_days_aveListedPrice_tractQuartile']\n",
        "data = data.drop(columns=columns_to_drop)\n",
        "\n",
        "# Create a mapping from existing data for Neighborhood\n",
        "zipcode_neighborhood_map = data.dropna(subset=['Zipcode', 'Neighborhood']).groupby('Zipcode')['Neighborhood'].agg(pd.Series.mode).to_dict()\n",
        "\n",
        "# Function to fill missing neighborhood values\n",
        "def fill_neighborhood(row):\n",
        "    if pd.isna(row['Neighborhood']) and row['Zipcode'] in zipcode_neighborhood_map:\n",
        "        return zipcode_neighborhood_map[row['Zipcode']]\n",
        "    else:\n",
        "        return row['Neighborhood']\n",
        "\n",
        "# Applying the function to fill missing Neighborhood values\n",
        "data['Neighborhood'] = data.apply(fill_neighborhood, axis=1)\n",
        "\n",
        "# Selecting features and target variable\n",
        "features = ['Property Type', 'Bedrooms', 'Bathrooms', 'Max Guests', 'Cleaning Fee (USD)',\n",
        "            'Minimum Stay', 'Number of Photos', 'host_is_superhost_in_period', 'numReviews_pastYear',\n",
        "            'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear', 'available_days', 'booked_days',\n",
        "            'booked_days_avePrice', 'Nightly Rate', 'occupancy_rate', 'City_x', 'Neighborhood', 'Zipcode']\n",
        "X = data[features]\n",
        "y = data['Rating Overall']  # Ensure this is the correct target column name\n",
        "\n",
        "# Preprocessing for categorical and numerical features\n",
        "numerical_features = ['Bedrooms', 'Bathrooms', 'Max Guests', 'Cleaning Fee (USD)',\n",
        "                      'Minimum Stay', 'Number of Photos', 'numReviews_pastYear',\n",
        "                      'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "                      'available_days', 'booked_days', 'booked_days_avePrice',\n",
        "                      'Nightly Rate', 'occupancy_rate']\n",
        "categorical_features = ['Property Type', 'City_x', 'Neighborhood', 'Zipcode',\n",
        "                        'host_is_superhost_in_period']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)])\n",
        "\n",
        "# Create a pipeline with 2-factor interaction polynomial features and a linear regression model\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('poly_features', PolynomialFeatures(interaction_only=True, include_bias=False)),\n",
        "                           ('regressor', LinearRegression())])\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Fitting the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predicting and evaluating the model\n",
        "y_pred = pipeline.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'R-squared: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "O-LtTEA-Fdzt",
        "outputId": "fb0c6f91-dd85-4470-ef71-17734591b389"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-90b434471641>\u001b[0m in \u001b[0;36m<cell line: 94>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Apply forward selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mselected_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# Fit the model with selected features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-90b434471641>\u001b[0m in \u001b[0;36mforward_selection\u001b[0;34m(X, y, feature_names, significance_level)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mp_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Exclude the constant term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mnew_pval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Get the p-value of the last added feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mmin_p_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_pval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/AirBnb Pre processed.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Dropping rows and columns as per your instructions\n",
        "columns_to_check = ['rating_ave_pastYear', 'numReviews_pastYear', 'numCancel_pastYear',\n",
        "                    'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "                    'prev_rating_ave_pastYear', 'prev_numReviews_pastYear',\n",
        "                    'prev_numCancel_pastYear', 'prev_num_5_star_Rev_pastYear',\n",
        "                    'prev_prop_5_StarReviews_pastYear', 'Property Type']\n",
        "columns_to_drop = ['Nightly Rate_tractQuartile', 'prev_Nightly Rate_tractQuartile',\n",
        "                   'available_days_aveListedPrice_tractQuartile',\n",
        "                   'prev_available_days_aveListedPrice_tractQuartile']\n",
        "\n",
        "data = data.dropna(subset=columns_to_check).drop(columns=columns_to_drop)\n",
        "\n",
        "# Selecting features and target variable\n",
        "features = ['Property Type', 'Bedrooms', 'Bathrooms', 'Max Guests', 'Cleaning Fee (USD)',\n",
        "            'Minimum Stay', 'Number of Photos', 'host_is_superhost_in_period', 'numReviews_pastYear',\n",
        "            'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear', 'available_days', 'booked_days',\n",
        "            'booked_days_avePrice', 'Nightly Rate', 'occupancy_rate', 'City_x', 'Neighborhood', 'Zipcode']\n",
        "X = data[features]\n",
        "y = data['Rating Overall']  # Ensure this is the correct target column name\n",
        "\n",
        "# Preprocessing for categorical and numerical features\n",
        "numerical_features = ['Bedrooms', 'Bathrooms', 'Max Guests', 'Cleaning Fee (USD)',\n",
        "                      'Minimum Stay', 'Number of Photos', 'numReviews_pastYear',\n",
        "                      'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "                      'available_days', 'booked_days', 'booked_days_avePrice',\n",
        "                      'Nightly Rate', 'occupancy_rate']\n",
        "categorical_features = ['Property Type', 'City_x', 'Neighborhood', 'Zipcode',\n",
        "                        'host_is_superhost_in_period']\n",
        "\n",
        "# Define transformers\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Preprocessing the data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)],\n",
        "    sparse_threshold=0)  # Ensuring the output is a dense matrix\n",
        "\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "y = y.values  # Ensure y is a numpy array\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Get feature names from the preprocessor\n",
        "def get_feature_names(column_transformer):\n",
        "    feature_names = []\n",
        "\n",
        "    for transformer in column_transformer.transformers_[:-1]:  # Last one is remainder and not used\n",
        "        transformed_names = transformer[1].get_feature_names_out(input_features=transformer[2])\n",
        "        feature_names.extend(transformed_names)\n",
        "\n",
        "    return feature_names\n",
        "\n",
        "feature_names = get_feature_names(preprocessor)\n",
        "\n",
        "# Function for forward selection\n",
        "def forward_selection(X, y, feature_names, significance_level=0.05):\n",
        "    initial_features = list(range(X.shape[1]))\n",
        "    best_features_indices = []\n",
        "\n",
        "    while len(initial_features) > 0:\n",
        "        remaining_features = list(set(initial_features) - set(best_features_indices))\n",
        "        new_pval = pd.Series(dtype=float)\n",
        "\n",
        "        for new_column in remaining_features:\n",
        "            current_features = best_features_indices + [new_column]\n",
        "            model = sm.OLS(y, sm.add_constant(X[:, current_features])).fit()\n",
        "            p_values = model.pvalues[1:]  # Exclude the constant term\n",
        "            new_pval.at[feature_names[new_column]] = p_values[-1]  # Get the p-value of the last added feature\n",
        "\n",
        "        min_p_value = new_pval.min()\n",
        "        if min_p_value < significance_level:\n",
        "            best_features_indices.append(new_pval.idxmin())\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return [feature_names[i] for i in best_features_indices]\n",
        "\n",
        "# Apply forward selection\n",
        "selected_features = forward_selection(X_train, y_train, feature_names)\n",
        "\n",
        "# Fit the model with selected features\n",
        "selected_feature_indices = [feature_names.index(f) for f in selected_features]\n",
        "model = sm.OLS(y_train, sm.add_constant(X_train[:, selected_feature_indices])).fit()\n",
        "\n",
        "# Print the summary of the model\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZhamHf7fSz4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/AirBnb Pre processed.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Drop rows where specified columns have NaN values\n",
        "columns_to_check = ['rating_ave_pastYear', 'numReviews_pastYear', 'numCancel_pastYear',\n",
        "                    'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "                    'prev_rating_ave_pastYear', 'prev_numReviews_pastYear',\n",
        "                    'prev_numCancel_pastYear', 'prev_num_5_star_Rev_pastYear',\n",
        "                    'prev_prop_5_StarReviews_pastYear', 'Property Type']\n",
        "data = data.dropna(subset=columns_to_check)\n",
        "\n",
        "# Drop specified columns\n",
        "columns_to_drop = ['Nightly Rate_tractQuartile', 'prev_Nightly Rate_tractQuartile',\n",
        "                   'available_days_aveListedPrice_tractQuartile',\n",
        "                   'prev_available_days_aveListedPrice_tractQuartile']\n",
        "data = data.drop(columns=columns_to_drop)\n",
        "\n",
        "# Create a mapping from existing data for Neighborhood\n",
        "zipcode_neighborhood_map = data.dropna(subset=['Zipcode', 'Neighborhood']).groupby('Zipcode')['Neighborhood'].agg(pd.Series.mode).to_dict()\n",
        "\n",
        "# Function to fill missing neighborhood values\n",
        "def fill_neighborhood(row):\n",
        "    if pd.isna(row['Neighborhood']) and row['Zipcode'] in zipcode_neighborhood_map:\n",
        "        return zipcode_neighborhood_map[row['Zipcode']]\n",
        "    else:\n",
        "        return row['Neighborhood']\n",
        "\n",
        "# Applying the function to fill missing Neighborhood values\n",
        "data['Neighborhood'] = data.apply(fill_neighborhood, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WW18xJThXpi",
        "outputId": "ec88d7e3-7949-474e-e4ea-62bcd0ad575d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Mean Squared Error: 7.102234942832142\n",
            "Training R-squared: 0.9663406218862001\n",
            "Testing Mean Squared Error: 47.22202835814408\n",
            "Testing R-squared: 0.7771738548075379\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/AirBnb Pre processed.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Dropping rows and columns as per your instructions\n",
        "columns_to_check = ['rating_ave_pastYear', 'numReviews_pastYear', 'numCancel_pastYear',\n",
        "                    'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "                    'prev_rating_ave_pastYear', 'prev_numReviews_pastYear',\n",
        "                    'prev_numCancel_pastYear', 'prev_num_5_star_Rev_pastYear',\n",
        "                    'prev_prop_5_StarReviews_pastYear', 'Property Type']\n",
        "columns_to_drop = ['Nightly Rate_tractQuartile', 'prev_Nightly Rate_tractQuartile',\n",
        "                   'available_days_aveListedPrice_tractQuartile',\n",
        "                   'prev_available_days_aveListedPrice_tractQuartile']\n",
        "\n",
        "data = data.dropna(subset=columns_to_check).drop(columns=columns_to_drop)\n",
        "\n",
        "# Selecting features and target variable\n",
        "features = ['Property Type', 'Bedrooms', 'Bathrooms', 'Max Guests', 'Cleaning Fee (USD)',\n",
        "            'Minimum Stay', 'Number of Photos', 'host_is_superhost_in_period', 'numReviews_pastYear',\n",
        "            'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear', 'available_days', 'booked_days',\n",
        "            'booked_days_avePrice', 'Nightly Rate', 'occupancy_rate', 'City_x', 'Neighborhood', 'Zipcode']\n",
        "X = data[features]\n",
        "y = data['Rating Overall']  # Ensure this is the correct target column name\n",
        "\n",
        "# Preprocessing for categorical and numerical features\n",
        "numerical_features = ['Bedrooms', 'Bathrooms', 'Max Guests', 'Cleaning Fee (USD)',\n",
        "                      'Minimum Stay', 'Number of Photos', 'numReviews_pastYear',\n",
        "                      'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "                      'available_days', 'booked_days', 'booked_days_avePrice',\n",
        "                      'Nightly Rate', 'occupancy_rate']\n",
        "categorical_features = ['Property Type', 'City_x', 'Neighborhood', 'Zipcode',\n",
        "                        'host_is_superhost_in_period']\n",
        "\n",
        "# Define transformers\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Preprocessing the data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)],\n",
        "    sparse_threshold=0)  # Ensuring the output is a dense matrix\n",
        "\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Fitting a Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting and evaluating the model\n",
        "y_train_pred = rf_model.predict(X_train)\n",
        "y_test_pred = rf_model.predict(X_test)\n",
        "\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(f'Training Mean Squared Error: {train_mse}')\n",
        "print(f'Training R-squared: {train_r2}')\n",
        "print(f'Testing Mean Squared Error: {test_mse}')\n",
        "print(f'Testing R-squared: {test_r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "aztMbLJ6lBzx",
        "outputId": "e49d744a-d3eb-4220-b2ec-97fb7d1ba979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1b94f5dfd941>\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Fit the random search model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mrf_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Best parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1705\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1706\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/AirBnb Pre processed.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Dropping rows and columns as per your instructions\n",
        "columns_to_check = ['rating_ave_pastYear', 'numReviews_pastYear', 'numCancel_pastYear',\n",
        "                    'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "                    'prev_rating_ave_pastYear', 'prev_numReviews_pastYear',\n",
        "                    'prev_numCancel_pastYear', 'prev_num_5_star_Rev_pastYear',\n",
        "                    'prev_prop_5_StarReviews_pastYear', 'Property Type']\n",
        "columns_to_drop = ['Nightly Rate_tractQuartile', 'prev_Nightly Rate_tractQuartile',\n",
        "                   'available_days_aveListedPrice_tractQuartile',\n",
        "                   'prev_available_days_aveListedPrice_tractQuartile']\n",
        "\n",
        "data = data.dropna(subset=columns_to_check).drop(columns=columns_to_drop)\n",
        "\n",
        "# Selecting features and target variable\n",
        "features = ['Property Type', 'Bedrooms', 'Bathrooms', 'Max Guests', 'Cleaning Fee (USD)',\n",
        "            'Minimum Stay', 'Number of Photos', 'host_is_superhost_in_period', 'numReviews_pastYear',\n",
        "            'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear', 'available_days', 'booked_days',\n",
        "            'booked_days_avePrice', 'Nightly Rate', 'occupancy_rate', 'City_x', 'Neighborhood', 'Zipcode']\n",
        "X = data[features]\n",
        "y = data['Rating Overall']\n",
        "\n",
        "# Preprocessing for categorical and numerical features\n",
        "numerical_features = ['Bedrooms', 'Bathrooms', 'Max Guests', 'Cleaning Fee (USD)',\n",
        "                      'Minimum Stay', 'Number of Photos', 'numReviews_pastYear',\n",
        "                      'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "                      'available_days', 'booked_days', 'booked_days_avePrice',\n",
        "                      'Nightly Rate', 'occupancy_rate']\n",
        "categorical_features = ['Property Type', 'City_x', 'Neighborhood', 'Zipcode',\n",
        "                        'host_is_superhost_in_period']\n",
        "\n",
        "# Define transformers\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Preprocessing the data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)],\n",
        "    sparse_threshold=0)\n",
        "\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid for RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Create a Random Forest Regressor\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Randomized search on hyper parameters\n",
        "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit the random search model\n",
        "rf_random.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters: \", rf_random.best_params_)\n",
        "\n",
        "# Using the best estimator found by RandomizedSearchCV\n",
        "best_rf = rf_random.best_estimator_\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_train_pred = best_rf.predict(X_train)\n",
        "y_test_pred = best_rf.predict(X_test)\n",
        "\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(f'Optimized Training Mean Squared Error: {train_mse}')\n",
        "print(f'Optimized Training R-squared: {train_r2}')\n",
        "print(f'Optimized Testing Mean Squared Error: {test_mse}')\n",
        "print(f'Optimized Testing R-squared: {test_r2}')\n",
        "\n",
        "# Feature Importance\n",
        "importances = best_rf.feature_importances_\n",
        "feature_importances = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
        "print(feature_importances)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnmA9SqwoNbW",
        "outputId": "fd91f369-7d29-4e14-ec20-e2108b8e2194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "427/427 [==============================] - 1s 1ms/step\n",
            "Ensemble Model Mean Squared Error: 63.98856292292871\n",
            "Ensemble Model R-squared: 0.6980577643894775\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/AirBnb Pre processed.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Dropping rows and columns as per your instructions\n",
        "columns_to_check = ['rating_ave_pastYear', 'numReviews_pastYear', 'numCancel_pastYear',\n",
        "                    'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "                    'prev_rating_ave_pastYear', 'prev_numReviews_pastYear',\n",
        "                    'prev_numCancel_pastYear', 'prev_num_5_star_Rev_pastYear',\n",
        "                    'prev_prop_5_StarReviews_pastYear', 'Property Type']\n",
        "columns_to_drop = ['Nightly Rate_tractQuartile', 'prev_Nightly Rate_tractQuartile',\n",
        "                   'available_days_aveListedPrice_tractQuartile',\n",
        "                   'prev_available_days_aveListedPrice_tractQuartile']\n",
        "\n",
        "data = data.dropna(subset=columns_to_check).drop(columns=columns_to_drop)\n",
        "\n",
        "# Selecting features and target variable\n",
        "features = ['Property Type', 'Bedrooms', 'Bathrooms', 'Max Guests', 'Cleaning Fee (USD)',\n",
        "            'Minimum Stay', 'Number of Photos', 'host_is_superhost_in_period', 'numReviews_pastYear',\n",
        "            'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear', 'available_days', 'booked_days',\n",
        "            'booked_days_avePrice', 'Nightly Rate', 'occupancy_rate', 'City_x', 'Neighborhood', 'Zipcode']\n",
        "X = data[features]\n",
        "y = data['Rating Overall']\n",
        "\n",
        "# Preprocessing for categorical and numerical features\n",
        "numerical_features = ['Bedrooms', 'Bathrooms', 'Max Guests', 'Cleaning Fee (USD)',\n",
        "                      'Minimum Stay', 'Number of Photos', 'numReviews_pastYear',\n",
        "                      'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "                      'available_days', 'booked_days', 'booked_days_avePrice',\n",
        "                      'Nightly Rate', 'occupancy_rate']\n",
        "categorical_features = ['Property Type', 'City_x', 'Neighborhood', 'Zipcode',\n",
        "                        'host_is_superhost_in_period']\n",
        "\n",
        "# Define transformers\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Preprocessing the data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)],\n",
        "    sparse_threshold=0)\n",
        "\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)\n",
        "ridge_model = Ridge(random_state=42)\n",
        "\n",
        "# Neural Network Model\n",
        "nn_model = Sequential()\n",
        "nn_model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "nn_model.add(Dense(64, activation='relu'))\n",
        "nn_model.add(Dense(1))\n",
        "nn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Fit models\n",
        "rf_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "nn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "# Predictions\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "ridge_pred = ridge_model.predict(X_test)\n",
        "nn_pred = nn_model.predict(X_test).flatten()\n",
        "\n",
        "# Average the predictions\n",
        "ensemble_pred = (rf_pred + xgb_pred + ridge_pred + nn_pred) / 4\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "ensemble_mse = mean_squared_error(y_test, ensemble_pred)\n",
        "ensemble_r2 = r2_score(y_test, ensemble_pred)\n",
        "\n",
        "print(f'Ensemble Model Mean Squared Error: {ensemble_mse}')\n",
        "print(f'Ensemble Model R-squared: {ensemble_r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "AkH_ChP3sKnr",
        "outputId": "0975417b-13af-446a-c817-0ac36cf631bd"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b7d895c2913a>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mxgb_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid_xgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mrf_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0mxgb_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1705\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1706\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/AirBnb Pre processed.csv'  # Update this with your file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Dropping rows and columns as per your instructions\n",
        "columns_to_check = ['rating_ave_pastYear', 'numReviews_pastYear', 'numCancel_pastYear',\n",
        "                    'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "                    'prev_rating_ave_pastYear', 'prev_numReviews_pastYear',\n",
        "                    'prev_numCancel_pastYear', 'prev_num_5_star_Rev_pastYear',\n",
        "                    'prev_prop_5_StarReviews_pastYear', 'Property Type']\n",
        "columns_to_drop = ['Nightly Rate_tractQuartile', 'prev_Nightly Rate_tractQuartile',\n",
        "                   'available_days_aveListedPrice_tractQuartile',\n",
        "                   'prev_available_days_aveListedPrice_tractQuartile']\n",
        "\n",
        "data = data.dropna(subset=columns_to_check).drop(columns=columns_to_drop)\n",
        "\n",
        "# Selecting features and target variable\n",
        "features = ['Property Type', 'Bedrooms', 'Bathrooms', 'Max Guests', 'Cleaning Fee (USD)',\n",
        "            'Minimum Stay', 'Number of Photos', 'host_is_superhost_in_period', 'numReviews_pastYear',\n",
        "            'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear', 'available_days', 'booked_days',\n",
        "            'booked_days_avePrice', 'Nightly Rate', 'occupancy_rate', 'City_x', 'Neighborhood', 'Zipcode']\n",
        "X = data[features]\n",
        "y = data['Rating Overall']\n",
        "\n",
        "# Preprocessing for categorical and numerical features\n",
        "numerical_features = ['Bedrooms', 'Bathrooms', 'Max Guests', 'Cleaning Fee (USD)',\n",
        "                      'Minimum Stay', 'Number of Photos', 'numReviews_pastYear',\n",
        "                      'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "                      'available_days', 'booked_days', 'booked_days_avePrice',\n",
        "                      'Nightly Rate', 'occupancy_rate']\n",
        "categorical_features = ['Property Type', 'City_x', 'Neighborhood', 'Zipcode',\n",
        "                        'host_is_superhost_in_period']\n",
        "\n",
        "# Define transformers\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Preprocessing the data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)],\n",
        "    sparse_threshold=0)\n",
        "\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Hyperparameter tuning for RandomForest and XGBoost\n",
        "param_grid_rf = {'n_estimators': [100, 200], 'max_depth': [10, 20]}\n",
        "param_grid_xgb = {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1]}\n",
        "\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)\n",
        "\n",
        "rf_grid = GridSearchCV(rf, param_grid_rf, cv=3, n_jobs=-1)\n",
        "xgb_grid = GridSearchCV(xgb_model, param_grid_xgb, cv=3, n_jobs=-1)\n",
        "\n",
        "rf_grid.fit(X_train, y_train)\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "\n",
        "# Neural Network Model\n",
        "nn_model = Sequential()\n",
        "nn_model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "nn_model.add(Dropout(0.3))\n",
        "nn_model.add(Dense(64, activation='relu'))\n",
        "nn_model.add(Dropout(0.3))\n",
        "nn_model.add(Dense(1))\n",
        "nn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "nn_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Stacking Ensemble\n",
        "estimators = [\n",
        "    ('rf', rf_grid.best_estimator_),\n",
        "    ('xgb', xgb_grid.best_estimator_)\n",
        "]\n",
        "stack_reg = StackingRegressor(estimators=estimators, final_estimator=Ridge())\n",
        "stack_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_pred_stack = stack_reg.predict(X_test)\n",
        "ensemble_mse = mean_squared_error(y_test, y_pred_stack)\n",
        "ensemble_r2 = r2_score(y_test, y_pred_stack)\n",
        "\n",
        "print(f'Stacking Ensemble Mean Squared Error: {ensemble_mse}')\n",
        "print(f'Stacking Ensemble R-squared: {ensemble_r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "oun7n1dhxaT7",
        "outputId": "45b60722-82f9-4428-9bc2-3dcbc5f42968"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-69a16b969d76>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Fit models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \"\"\"\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1248\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'AirBnb Pre processed.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/AirBnb Pre processed.csv'  # Update this with your file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Dropping rows and columns as per your instructions\n",
        "columns_to_check = ['rating_ave_pastYear', 'numReviews_pastYear', 'numCancel_pastYear',\n",
        "                    'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "                    'prev_rating_ave_pastYear', 'prev_numReviews_pastYear',\n",
        "                    'prev_numCancel_pastYear', 'prev_num_5_star_Rev_pastYear',\n",
        "                    'prev_prop_5_StarReviews_pastYear', 'Property Type']\n",
        "columns_to_drop = ['Nightly Rate_tractQuartile', 'prev_Nightly Rate_tractQuartile',\n",
        "                   'available_days_aveListedPrice_tractQuartile',\n",
        "                   'prev_available_days_aveListedPrice_tractQuartile']\n",
        "\n",
        "data = data.dropna(subset=columns_to_check).drop(columns=columns_to_drop)\n",
        "\n",
        "# Assuming the target variable is 'rating_overall'\n",
        "y = data['Rating Overall']\n",
        "X = data.drop('Rating Overall', axis=1)\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Preprocessing for categorical and numerical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "gb_model = GradientBoostingRegressor(random_state=42)\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Fit models\n",
        "rf_model.fit(X_train, y_train)\n",
        "gb_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Neural Network Model\n",
        "nn_model = Sequential()\n",
        "nn_model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "nn_model.add(Dropout(0.3))\n",
        "nn_model.add(Dense(64, activation='relu'))\n",
        "nn_model.add(Dropout(0.3))\n",
        "nn_model.add(Dense(1))\n",
        "nn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
        "nn_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stop], verbose=1)\n",
        "\n",
        "# Predictions and evaluation\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "gb_pred = gb_model.predict(X_test)\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "nn_pred = nn_model.predict(X_test).flatten()\n",
        "\n",
        "# Averaging predictions\n",
        "ensemble_pred = (rf_pred + gb_pred + xgb_pred + nn_pred) / 4\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "ensemble_mse = mean_squared_error(y_test, ensemble_pred)\n",
        "ensemble_r2 = r2_score(y_test, ensemble_pred)\n",
        "\n",
        "print(f'Ensemble Model Mean Squared Error: {ensemble_mse}')\n",
        "print(f'Ensemble Model R-squared: {ensemble_r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "y9q6SQq73Uo5",
        "outputId": "3ecd1c26-7bc8-488e-9700-ad8d6c5b5bf7"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0aad964383db>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Fit models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m         array = _ensure_sparse_format(\n\u001b[0m\u001b[1;32m    846\u001b[0m             \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    559\u001b[0m             )\n\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    562\u001b[0m                 \u001b[0mspmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                 \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'AirBnb Pre processed.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Create a mapping from existing data for Neighborhood\n",
        "zipcode_neighborhood_map = data.dropna(subset=['Zipcode', 'Neighborhood']).groupby('Zipcode')['Neighborhood'].agg(pd.Series.mode).to_dict()\n",
        "\n",
        "# Function to fill missing neighborhood values\n",
        "def fill_neighborhood(row):\n",
        "    if pd.isna(row['Neighborhood']) and row['Zipcode'] in zipcode_neighborhood_map:\n",
        "        return zipcode_neighborhood_map[row['Zipcode']]\n",
        "    else:\n",
        "        return row['Neighborhood']\n",
        "\n",
        "# Applying the function to fill missing Neighborhood values\n",
        "data['Neighborhood'] = data.apply(fill_neighborhood, axis=1)\n",
        "\n",
        "# Dropping rows and columns as per your instructions\n",
        "columns_to_check = ['rating_ave_pastYear', 'numReviews_pastYear', 'numCancel_pastYear',\n",
        "                    'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "                    'prev_rating_ave_pastYear', 'prev_numReviews_pastYear',\n",
        "                    'prev_numCancel_pastYear', 'prev_num_5_star_Rev_pastYear',\n",
        "                    'prev_prop_5_StarReviews_pastYear', 'Property Type']\n",
        "columns_to_drop = ['Nightly Rate_tractQuartile', 'prev_Nightly Rate_tractQuartile',\n",
        "                   'available_days_aveListedPrice_tractQuartile',\n",
        "                   'prev_available_days_aveListedPrice_tractQuartile']\n",
        "\n",
        "# Assuming the target variable is 'rating_overall'\n",
        "y = data['Rating Overall']\n",
        "X = data.drop('Rating Overall', axis=1)\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Preprocessing for categorical and numerical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)])\n",
        "\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Fit models\n",
        "rf_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and evaluation\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Averaging predictions\n",
        "ensemble_pred = (rf_pred + xgb_pred) / 2\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "ensemble_mse = mean_squared_error(y_test, ensemble_pred)\n",
        "ensemble_r2 = r2_score(y_test, ensemble_pred)\n",
        "\n",
        "print(f'Ensemble Model Mean Squared Error: {ensemble_mse}')\n",
        "print(f'Ensemble Model R-squared: {ensemble_r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "qO4KR3SGDrzj",
        "outputId": "91739446-4762-4b67-cd22-163d00a1319a"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-71cfe89917c7>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0;34m'prev_numCancel_pastYear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prev_num_5_star_Rev_pastYear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     'prev_prop_5_StarReviews_pastYear', 'Property Type']\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns_to_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Drop specified columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdropna\u001b[0;34m(self, axis, how, thresh, subset, inplace)\u001b[0m\n\u001b[1;32m   6559\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6560\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6561\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6562\u001b[0m             \u001b[0magg_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ['rating_ave_pastYear', 'numReviews_pastYear', 'numCancel_pastYear', 'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear', 'prev_rating_ave_pastYear', 'prev_numReviews_pastYear', 'prev_numCancel_pastYear', 'prev_num_5_star_Rev_pastYear', 'prev_prop_5_StarReviews_pastYear']"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/AirBnb Pre processed Tableau 1.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Drop rows where specified columns have NaN values\n",
        "columns_to_check = ['rating_ave_pastYear', 'numReviews_pastYear', 'numCancel_pastYear',\n",
        "                    'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "                    'prev_rating_ave_pastYear', 'prev_numReviews_pastYear',\n",
        "                    'prev_numCancel_pastYear', 'prev_num_5_star_Rev_pastYear',\n",
        "                    'prev_prop_5_StarReviews_pastYear', 'Property Type']\n",
        "data = data.dropna(subset=columns_to_check)\n",
        "\n",
        "# Drop specified columns\n",
        "columns_to_drop = ['Nightly Rate_tractQuartile', 'prev_Nightly Rate_tractQuartile',\n",
        "                   'available_days_aveListedPrice_tractQuartile',\n",
        "                   'prev_available_days_aveListedPrice_tractQuartile']\n",
        "data = data.drop(columns=columns_to_drop)\n",
        "\n",
        "# Create a mapping from existing data for Neighborhood\n",
        "zipcode_neighborhood_map = data.dropna(subset=['Zipcode', 'Neighborhood']).groupby('Zipcode')['Neighborhood'].agg(pd.Series.mode).to_dict()\n",
        "\n",
        "# Function to fill missing neighborhood values\n",
        "def fill_neighborhood(row):\n",
        "    if pd.isna(row['Neighborhood']) and row['Zipcode'] in zipcode_neighborhood_map:\n",
        "        return zipcode_neighborhood_map[row['Zipcode']]\n",
        "    else:\n",
        "        return row['Neighborhood']\n",
        "\n",
        "# Applying the function to fill missing Neighborhood values\n",
        "data['Neighborhood'] = data.apply(fill_neighborhood, axis=1)\n",
        "\n",
        "# Import necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Calculate Occupancy Rate\n",
        "df['occupancy_rate'] = df['booked_days'] / (df['booked_days'] + df['available_days'])\n",
        "\n",
        "# Selecting relevant variables\n",
        "features = ['Cleaning Fee (USD)', 'host_is_superhost_in_period', 'prev_host_is_superhost_in_period',\n",
        "            'rating_ave_pastYear', 'numReviews_pastYear', 'num_5_star_Rev_pastYear',\n",
        "            'prop_5_StarReviews_pastYear', 'numReservedDays_pastYear', 'numReserv_pastYear',\n",
        "            'numCancel_pastYear', 'available_days', 'available_days_aveListedPrice',\n",
        "            'booked_days', 'booked_days_avePrice', 'occupancy_rate']\n",
        "\n",
        "# Assuming 'revenue' is your target variable\n",
        "target = 'revenue'\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
        "\n",
        "# Model training and evaluation\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Random Forest': RandomForestRegressor(),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(),\n",
        "    'Lasso': Lasso(),\n",
        "    'Ridge': Ridge()\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    print(f'{name} - RMSE: {np.sqrt(mean_squared_error(y_test, predictions))}, R2: {r2_score(y_test, predictions)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "zwLQcM6CIZ_H",
        "outputId": "a72b9487-04de-4cfb-cf34-7952706c2f79"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'revenue'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a83a6cee8326>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Prepare the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Handle missing values if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'revenue'"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/AirBnb Pre processed Tableau 1.csv')\n",
        "\n",
        "# Selecting relevant variables (replace with exact names as needed)\n",
        "features = ['Cleaning Fee (USD)', 'Host Is Superhost In Period', 'rating ave pastYear', 'numReviews pastYear',\n",
        "            'Booked Days', 'available days aveListedPrice', 'Nightly Rate']\n",
        "\n",
        "# Assuming 'Revenue' and 'Occupancy Rate' are your target variables\n",
        "y_revenue = df['Revenue']\n",
        "y_occupancy_rate = df['Occupancy Rate']\n",
        "\n",
        "\n",
        "# Prepare the data\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Handle missing values if needed\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "y.fillna(y.mean(), inplace=True)\n",
        "# Train-test split for both targets\n",
        "X_train_revenue, X_test_revenue, y_train_revenue, y_test_revenue = train_test_split(df[features], y_revenue, test_size=0.2, random_state=42)\n",
        "X_train_occupancy, X_test_occupancy, y_train_occupancy, y_test_occupancy = train_test_split(df[features], y_occupancy_rate, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Function to train and evaluate models\n",
        "def train_and_evaluate(models, X_train, X_test, y_train, y_test, target_name):\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        predictions = model.predict(X_test)\n",
        "        print(f'{name} - {target_name} - RMSE: {np.sqrt(mean_squared_error(y_test, predictions))}, R2: {r2_score(y_test, predictions)}')\n",
        "\n",
        "# Models to evaluate\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Random Forest': RandomForestRegressor(),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(),\n",
        "    'Lasso': Lasso(),\n",
        "    'Ridge': Ridge()\n",
        "}\n",
        "\n",
        "# Evaluating models for Revenue\n",
        "print(\"Evaluating models for Revenue:\")\n",
        "train_and_evaluate(models, X_train_revenue, X_test_revenue, y_train_revenue, y_test_revenue, \"Revenue\")\n",
        "\n",
        "# Evaluating models for Occupancy Rate\n",
        "print(\"\\nEvaluating models for Occupancy Rate:\")\n",
        "train_and_evaluate(models, X_train_occupancy, X_test_occupancy, y_train_occupancy, y_test_occupancy, \"Occupancy Rate\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08KN2VuiIh7-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures, MaxAbsScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        " \n",
        "# Function to calculate MAPE\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    non_zero_mask = y_true != 0\n",
        "    return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
        " \n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/AirbnbHouston_Preprocessed_dataset 2.csv')  # Adjust path as needed\n",
        " \n",
        "# Define the target variable and features\n",
        "target = 'occupancy_rate'\n",
        "features = ['host_is_superhost_in_period', 'numReviews_pastYear', 'booked_days',\n",
        "            'rating_ave_pastYear', 'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "            'prev_rating_ave_pastYear', 'prev_prop_5_StarReviews_pastYear', 'Minimum Stay',\n",
        "            'Max Guests', 'Zipcode', 'Pets Allowed', 'available_days', 'Nightly Rate',\n",
        "            'Cleaning Fee (USD)', 'Bedrooms', 'Bathrooms', 'Rating Overall', 'superhost_change_lose_superhost','superhost_change_gain_superhost']\n",
        " \n",
        "X = data[features]\n",
        "y = data[target]\n",
        " \n",
        "# Splitting the dataset into training and validation sets (60:40)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        " \n",
        "# Preprocessing\n",
        "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        " \n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', MaxAbsScaler())])\n",
        " \n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        " \n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)])\n",
        " \n",
        "# Fit and transform training data\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_val_processed = preprocessor.transform(X_val)\n",
        " \n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "X_train_poly = poly_features.fit_transform(X_train_processed)\n",
        "X_val_poly = poly_features.transform(X_val_processed)\n",
        " \n",
        "# Linear Regression Model\n",
        "linear_reg = LinearRegression()\n",
        "linear_reg.fit(X_train_poly, y_train)\n",
        " \n",
        "# Predict and evaluate on training and validation data\n",
        "y_train_pred = linear_reg.predict(X_train_poly)\n",
        "y_val_pred = linear_reg.predict(X_val_poly)\n",
        " \n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "val_r2 = r2_score(y_val, y_val_pred)\n",
        " \n",
        "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
        "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
        " \n",
        "# Print model summary\n",
        "print(\"Model Summary:\")\n",
        " \n",
        "print(\"Training MSE:\", train_mse)\n",
        "print(\"Training R-squared:\", train_r2)\n",
        "print(\"Training MAPE:\", train_mape, \"%\")\n",
        "print(\"Validation MSE:\", val_mse)\n",
        "print(\"Validation R-squared:\", val_r2)\n",
        "print(\"Validation MAPE:\", val_mape, \"%\")\n",
        " \n",
        "print(\"Intercept:\", linear_reg.intercept_)\n",
        "print(\"Coefficients:\", linear_reg.coef_)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
