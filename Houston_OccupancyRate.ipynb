{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jawnZhiRTWRK",
        "outputId": "4d2a6176-1abd-40ef-9aed-3f2c33ea558e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows: 47606\n",
            "Number of columns: 111\n",
            "45 111\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('airbnb_Boston.csv')\n",
        "\n",
        "# Count number of rows and columns\n",
        "num_rows = len(df)\n",
        "num_cols = len(df.columns)\n",
        "\n",
        "# Print number of rows and columns\n",
        "print(f'Number of rows: {num_rows}')\n",
        "print(f'Number of columns: {num_cols}')\n",
        "c=0\n",
        "p=0\n",
        "# Iterate through columns, count length, and calculate % missing\n",
        "for col in df.columns:\n",
        "    p=p+1\n",
        "    col_len = len(df[col].dropna())\n",
        "    pct_missing = round(100*(num_rows - col_len)/num_rows, 2)\n",
        "\n",
        "    # print(f'{col}:')\n",
        "    # print(f'  Column length: {col_len}')\n",
        "    # print(f'  Percent missing: {pct_missing}%')\n",
        "\n",
        "    if pct_missing > .85:\n",
        "      c=c+1\n",
        "      #print(f'*** {col} has more than 65% missing values ***')\n",
        "print(c, p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbq_kJ72Wvt2"
      },
      "outputs": [],
      "source": [
        "num_rows = len(df)\n",
        "\n",
        "highly_missing_cols = []\n",
        "\n",
        "print('Column Names | Non-Null Cells | Pct Missing')\n",
        "print('--------------+----------------+------------')\n",
        "\n",
        "for col in df.columns:\n",
        "    non_null = df[col].notnull().sum()\n",
        "\n",
        "    pct_missing = 100*(num_rows - non_null)/num_rows\n",
        "\n",
        "    print(f'{col:15} | {non_null:12,} | {pct_missing:6.2f}%')\n",
        "\n",
        "    if pct_missing > 60:\n",
        "        highly_missing_cols.append(col)\n",
        "\n",
        "print(f'\\nTotal Rows: {num_rows:,}')\n",
        "\n",
        "print('\\nColumns with >80% missing values:')\n",
        "for col in highly_missing_cols:\n",
        "    print(f'- {col}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9ApIRmOfEvT",
        "outputId": "1a8e73d8-b15d-4c54-cc59-5ed65e2c4c7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Airbnb Host ID', 'Airbnb Property ID', 'City_x',\n",
            "       'superhost_period_all', 'Scraped Date', 'host_is_superhost_in_period',\n",
            "       'prev_host_is_superhost_in_period', 'Superhost',\n",
            "       'prev_host_is_superhost', 'superhost_change',\n",
            "       'superhost_change_lose_superhost', 'superhost_change_gain_superhost',\n",
            "       'rating_ave_pastYear', 'numReviews_pastYear', 'numCancel_pastYear',\n",
            "       'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
            "       'prev_rating_ave_pastYear', 'prev_numReviews_pastYear',\n",
            "       'prev_numCancel_pastYear', 'prev_num_5_star_Rev_pastYear',\n",
            "       'prev_prop_5_StarReviews_pastYear', 'numReservedDays_pastYear',\n",
            "       'numReserv_pastYear', 'prev_numReservedDays_pastYear',\n",
            "       'prev_numReserv_pastYear', 'available_days',\n",
            "       'available_days_aveListedPrice', 'booked_days', 'booked_days_avePrice',\n",
            "       'prev_available_days', 'prev_available_days_aveListedPrice',\n",
            "       'prev_booked_days', 'prev_booked_days_avePrice', 'Property Type',\n",
            "       'Listing Type', 'Created Date', 'Zipcode', 'Bedrooms', 'Bathrooms',\n",
            "       'Neighborhood', 'Max Guests', 'Cleaning Fee (USD)', 'Minimum Stay',\n",
            "       'Number of Photos', 'Latitude', 'Longitude', 'Pets Allowed',\n",
            "       'Instantbook Enabled', 'prev_Instantbook Enabled', 'Nightly Rate',\n",
            "       'prev_Nightly Rate', 'Number of Reviews', 'prev_Number of Reviews',\n",
            "       'Rating Overall', 'prev_Rating Overall', 'revenue', 'occupancy_rate',\n",
            "       'prev_revenue', 'prev_occupancy_rate', 'census_tract',\n",
            "       'tract_total_pop', 'tract_white_perc', 'tract_black_perc',\n",
            "       'tract_asian_perc', 'tract_housing_units', 'zip_total_population',\n",
            "       'zip_hispanic_or_latino_anyrace',\n",
            "       'zip_hispanic_or_latino_anyrace_percent', 'zip_white_nothispanic',\n",
            "       'zip_white_nothispanic_percent', 'zip_black_nothispanic',\n",
            "       'zip_black_nothispanic_percent', 'zip_asian_nothispanic',\n",
            "       'zip_asian_nothispanic_percent', 'tract_count_obs',\n",
            "       'tract_unique_prices', 'Nightly Rate_tractQuartile',\n",
            "       'prev_Nightly Rate_tractQuartile',\n",
            "       'available_days_aveListedPrice_tractQuartile',\n",
            "       'prev_available_days_aveListedPrice_tractQuartile', 'tract_superhosts',\n",
            "       'tract_superhosts_ratio', 'tract_prev_superhosts',\n",
            "       'tract_prev_superhosts_ratio', 'tract_price_variance',\n",
            "       'tractQuartilePrice_variance', 'prev_host_is_superhost1',\n",
            "       'prev_year_superhosts', 'booked_days_period_city',\n",
            "       'revenue_period_city', 'booked_days_period_tract',\n",
            "       'revenue_period_tract', 'tract_booking_share', 'tract_revenue_share'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of valid columns\n",
        "valid_cols = ['Airbnb Host ID', 'Airbnb Property ID', 'City_x', 'superhost_period_all',\n",
        "              'Scraped Date', 'host_is_superhost_in_period', 'prev_host_is_superhost_in_period',\n",
        "              'Superhost','prev_host_is_superhost', 'superhost_change',\n",
        "              'superhost_change_lose_superhost', 'superhost_change_gain_superhost',\n",
        "              'rating_ave_pastYear', 'numReviews_pastYear', 'numCancel_pastYear',\n",
        "              'num_5_star_Rev_pastYear','prop_5_StarReviews_pastYear',\n",
        "              'prev_rating_ave_pastYear', 'prev_numReviews_pastYear','prev_numCancel_pastYear',\n",
        "              'prev_num_5_star_Rev_pastYear','prev_prop_5_StarReviews_pastYear','numReservedDays_pastYear',\n",
        "              'numReserv_pastYear','prev_numReservedDays_pastYear','prev_numReserv_pastYear',\n",
        "              'available_days','available_days_aveListedPrice','booked_days','booked_days_avePrice',\n",
        "              'prev_available_days','prev_available_days_aveListedPrice','prev_available_days','prev_available_days_aveListedPrice',\n",
        "              'prev_booked_days','prev_booked_days_avePrice','Property Type','Listing Type','Created Date',\n",
        "              'Zipcode','Bedrooms','Bathrooms','Neighborhood','Max Guests','Cleaning Fee (USD)',\n",
        "              'Minimum Stay','Number of Photos','Latitude','Longitude','Pets Allowed',\n",
        "              'Instantbook Enabled','prev_Instantbook Enabled', 'Nightly Rate','prev_Nightly Rate',\n",
        "              'Number of Reviews','prev_Number of Reviews','Rating Overall','prev_Rating Overall','revenue',\n",
        "              'occupancy_rate','prev_revenue','prev_occupancy_rate','census_tract','tract_total_pop',\n",
        "              'tract_white_perc','tract_black_perc','tract_asian_perc','tract_housing_units', 'zip_total_population',\n",
        "              'zip_hispanic_or_latino_anyrace','zip_hispanic_or_latino_anyrace_percent','zip_white_nothispanic',\n",
        "              'zip_white_nothispanic_percent','zip_black_nothispanic','zip_black_nothispanic_percent',\n",
        "              'zip_asian_nothispanic','zip_asian_nothispanic_percent','tract_count_obs','tract_unique_prices',\n",
        "              'Nightly Rate_tractQuartile','prev_Nightly Rate_tractQuartile','available_days_aveListedPrice_tractQuartile',\n",
        "              'prev_available_days_aveListedPrice_tractQuartile','tract_superhosts','tract_superhosts_ratio',\n",
        "              'tract_prev_superhosts','tract_prev_superhosts_ratio','tract_price_variance','tractQuartilePrice_variance',\n",
        "              'prev_host_is_superhost1','prev_year_superhosts','booked_days_period_city','revenue_period_city',\n",
        "              'booked_days_period_tract','revenue_period_tract','tract_booking_share','tract_revenue_share','year']\n",
        "\n",
        "# Drop invalid columns\n",
        "boston_df=df\n",
        "df.drop(columns=[col for col in df.columns if col not in valid_cols], inplace=True)\n",
        "\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSxVWCVKiF9O"
      },
      "source": [
        "1. Exploratory Data Analysis (EDA)\n",
        "2. Feature Engineering & Data Preprocessing\n",
        "3. Train-Test Split\n",
        "4. Model Selection (Linear Regression/ Lasso Regression/Ridge Regression/ Random Forest Regressor/Gradient Boosting Regressor/ Support Vector Regression)\n",
        "5. Model Evaluation [Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAWSVqqhkRhm",
        "outputId": "cd29418b-5d71-406e-848b-b99f19529dc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zipcodes with missing Neighborhoods after update:\n",
            "[]\n",
            "Number of missing Neighborhood values after fill: 0\n"
          ]
        }
      ],
      "source": [
        "#categorical preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'cleaned_data 1.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# # Drop rows where specified columns have NaN values\n",
        "# columns_to_check = ['rating_ave_pastYear', 'numReviews_pastYear', 'numCancel_pastYear',\n",
        "#                     'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "#                     'prev_rating_ave_pastYear', 'prev_numReviews_pastYear',\n",
        "#                     'prev_numCancel_pastYear', 'prev_num_5_star_Rev_pastYear',\n",
        "#                     'prev_prop_5_StarReviews_pastYear', 'Property Type']\n",
        "# data = data.dropna(subset=columns_to_check)\n",
        "\n",
        "# # Drop specified columns\n",
        "# columns_to_drop = ['Nightly Rate_tractQuartile', 'prev_Nightly Rate_tractQuartile',\n",
        "#                    'available_days_aveListedPrice_tractQuartile',\n",
        "#                    'prev_available_days_aveListedPrice_tractQuartile']\n",
        "# data = data.drop(columns=columns_to_drop)\n",
        "cleaned_data=data\n",
        "zipcode_neighborhood_dict = cleaned_data.groupby('Zipcode')['Neighborhood'].first().to_dict()\n",
        "# Update the Zipcode dictionary\n",
        "zipcode_neighborhood_dict.update({2163: \"Allston\", 22713: \"Culpeper County\"})\n",
        "\n",
        "# Replace missing Neighborhood values in cleaned_data\n",
        "cleaned_data['Neighborhood'] = cleaned_data.apply(\n",
        "    lambda row: zipcode_neighborhood_dict[row['Zipcode']] if pd.isnull(row['Neighborhood']) else row['Neighborhood'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Verify if there are still missing values\n",
        "missing_neighborhoods_after_update = cleaned_data.loc[cleaned_data['Neighborhood'].isnull(), 'Zipcode'].unique()\n",
        "\n",
        "# Print the Zipcodes with missing Neighborhoods after the update\n",
        "print(\"Zipcodes with missing Neighborhoods after update:\")\n",
        "print(missing_neighborhoods_after_update)\n",
        "# Replace missing Neighborhood values with values from the dictionary\n",
        "cleaned_data['Neighborhood'] = cleaned_data.apply(\n",
        "    lambda row: zipcode_neighborhood_dict.get(row['Zipcode'], row['Neighborhood']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "\n",
        "# Verify that missing values have been filled\n",
        "missing_neighborhoods_after_fill = cleaned_data['Neighborhood'].isnull().sum()\n",
        "print(\"Number of missing Neighborhood values after fill:\", missing_neighborhoods_after_fill)\n",
        "\n",
        "# Assuming 'cleaned_data' is your DataFrame\n",
        "#cleaned_data.to_csv('cleaned_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYzrBt1ukFdm",
        "outputId": "6c6f2da4-0fa5-4bbf-b8fd-cd8fc2bcc933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape before removing outliers: (45504, 93)\n",
            "Shape after removing outliers: (22078, 93)\n"
          ]
        }
      ],
      "source": [
        "# import pandas as pd\n",
        "# from scipy.stats import zscore\n",
        "\n",
        "# # Assuming 'cleaned_data' is your DataFrame\n",
        "# numeric_data = cleaned_data.select_dtypes(include=['number'])\n",
        "\n",
        "# # Calculate z-scores for each numeric column\n",
        "# z_scores = zscore(numeric_data)\n",
        "\n",
        "# # Define a threshold for outliers (e.g., mean +/- 1.5 SD)\n",
        "# threshold = 3\n",
        "# outlier_mask = (z_scores > threshold) | (z_scores < -threshold)\n",
        "\n",
        "# # Remove rows containing outliers\n",
        "# cleaned_data_no_outliers = cleaned_data[~outlier_mask.any(axis=1)]\n",
        "\n",
        "# # Display the shape before and after removing outliers\n",
        "# print(\"Shape before removing outliers:\", cleaned_data.shape)\n",
        "# print(\"Shape after removing outliers:\", cleaned_data_no_outliers.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P2vFfwzhbTS",
        "outputId": "a06ae274-f231-4f83-84b4-c48af63512d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (31852, 92)\n",
            "X_test shape: (13652, 92)\n",
            "y_train shape: (31852,)\n",
            "y_test shape: (13652,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "#cleaned_data=df\n",
        "\n",
        "# Assuming 'Rating Overall' is your target variable\n",
        "X = cleaned_data.drop(['Rating Overall'], axis=1)  # Features\n",
        "y = cleaned_data['Rating Overall']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Display the shape of the resulting datasets\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrRNb5JLOuNI"
      },
      "outputs": [],
      "source": [
        "# Update the column names based on your specific dataset\n",
        "sampled_data = cleaned_data.sample(n=10000, random_state=42)\n",
        "cleaned_data=sampled_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FsjOQleg-os"
      },
      "source": [
        "Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlweDP4rSwc1",
        "outputId": "b7f4e3c7-4db3-4e74-850a-8a165529636a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-48f890d3e261>:19: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  categorical_features = X.select_dtypes(include=[np.object]).columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear Regression Model on Training Set:\n",
            "RMSE: 5.064688873857292\n",
            "R-squared: 0.8842550531065383\n"
          ]
        }
      ],
      "source": [
        "#linear regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Assuming your data is in a DataFrame called 'cleaned_data'\n",
        "# Make sure to replace 'Rating_Overall' with the actual column name if needed\n",
        "X = sampled_data.drop('Rating Overall', axis=1, errors='ignore')  # Independent variables\n",
        "y = sampled_data['Rating Overall']  # Dependent variable\n",
        "\n",
        "# Identify numeric and categorical features\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
        "categorical_features = X.select_dtypes(include=[np.object]).columns\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a preprocessing pipeline\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Append the linear regression model to the preprocessor\n",
        "model_linear = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                ('regressor', LinearRegression())])\n",
        "\n",
        "# Fit the model\n",
        "model_linear.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on the training set\n",
        "y_pred_train = model_linear.predict(X_train)\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "rmse_train = np.sqrt(mse_train)\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "\n",
        "# Display results\n",
        "print(\"Linear Regression Model on Training Set:\")\n",
        "print(\"RMSE:\", rmse_train)\n",
        "print(\"R-squared:\", r2_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOkaHIJcVQ_H"
      },
      "outputs": [],
      "source": [
        "#checking for coeff of linear regrssion\n",
        "# Access the 'regressor' step from the pipeline\n",
        "linear_regressor = model_linear.named_steps['regressor']\n",
        "\n",
        "# Print the intercept and coefficients\n",
        "print(\"Intercept:\", linear_regressor.intercept_)\n",
        "print(\"Coefficients:\")\n",
        "for feature, coefficient in zip(X_train.columns, linear_regressor.coef_):\n",
        "    print(f\"{feature}: {coefficient}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94XDVLHae-j5"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "# Convert to NumPy array\n",
        "X_train_with_const_np = X_train_with_const.to_numpy()\n",
        "\n",
        "# Fit the model using statsmodels\n",
        "stats_model = sm.OLS(y_train, X_train_with_const_np).fit()\n",
        "\n",
        "# Display detailed summary\n",
        "print(stats_model.summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMLagGQnQcIC"
      },
      "source": [
        "# neighbourhood still has missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0kvfYkkQfAm"
      },
      "outputs": [],
      "source": [
        "# column_name = 'Neighborhood'\n",
        "# df=cleaned_data\n",
        "# # Find rows with missing values in the specified column\n",
        "# rows_with_missing_values = df[df[column_name].isnull()]\n",
        "\n",
        "# # Display the rows with missing values\n",
        "# print(\"Rows with missing values in column '{}':\".format(column_name))\n",
        "# print(rows_with_missing_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfR7kItPR41c"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Assuming your DataFrame is named 'cleaned_data'\n",
        "# # Create a dictionary mapping Zipcode to Neighborhood\n",
        "# zipcode_neighborhood_dict = cleaned_data.groupby('Zipcode')['Neighborhood'].first().to_dict()\n",
        "# # Update the Zipcode dictionary\n",
        "# zipcode_neighborhood_dict.update({2163: \"Allston\", 22713: \"Culpeper County\"})\n",
        "\n",
        "# # Replace missing Neighborhood values in cleaned_data\n",
        "# cleaned_data['Neighborhood'] = cleaned_data.apply(\n",
        "#     lambda row: zipcode_neighborhood_dict[row['Zipcode']] if pd.isnull(row['Neighborhood']) else row['Neighborhood'],\n",
        "#     axis=1\n",
        "# )\n",
        "\n",
        "# # Verify if there are still missing values\n",
        "# missing_neighborhoods_after_update = cleaned_data.loc[cleaned_data['Neighborhood'].isnull(), 'Zipcode'].unique()\n",
        "\n",
        "# # Print the Zipcodes with missing Neighborhoods after the update\n",
        "# print(\"Zipcodes with missing Neighborhoods after update:\")\n",
        "# print(missing_neighborhoods_after_update)\n",
        "# # Replace missing Neighborhood values with values from the dictionary\n",
        "# cleaned_data['Neighborhood'] = cleaned_data.apply(\n",
        "#     lambda row: zipcode_neighborhood_dict.get(row['Zipcode'], row['Neighborhood']),\n",
        "#     axis=1\n",
        "# )\n",
        "\n",
        "\n",
        "# # Verify that missing values have been filled\n",
        "# missing_neighborhoods_after_fill = cleaned_data['Neighborhood'].isnull().sum()\n",
        "# print(\"Number of missing Neighborhood values after fill:\", missing_neighborhoods_after_fill)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3sQGHVQa8mq",
        "outputId": "e015d991-e4ec-4099-d050-5b0695eaf2b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-29-ad44ed6b94ea>:13: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  categorical_features = X.select_dtypes(include=[np.object]).columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear Regression Model on Training Set:\n",
            "RMSE: 9.992761000997206\n",
            "R-squared: 0.5267593372793047\n"
          ]
        }
      ],
      "source": [
        "#linear reg after missing values treated\n",
        "\n",
        "# Assuming your data is in a DataFrame called 'cleaned_data'\n",
        "# Make sure to replace 'Rating_Overall' with the actual column name if needed\n",
        "# Assuming your data is in a DataFrame called 'cleaned_data'\n",
        "# Update the column names based on your specific dataset\n",
        "columns_to_remove = ['Rating Overall', 'prev_rating_ave_pastYear','prev_Rating Overall','num_5_star_Rev_pastYear', 'prev_num_5_star_Rev_pastYear','rating_ave_pastYear']\n",
        "X = cleaned_data.drop(columns=columns_to_remove, errors='ignore') # Independent variables\n",
        "y = cleaned_data['Rating Overall']  # Dependent variable\n",
        "\n",
        "# Identify numeric and categorical features\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
        "categorical_features = X.select_dtypes(include=[np.object]).columns\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a preprocessing pipeline\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Append the linear regression model to the preprocessor\n",
        "model_linear = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                ('regressor', LinearRegression())])\n",
        "\n",
        "# Fit the model\n",
        "model_linear.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on the training set\n",
        "y_pred_train = model_linear.predict(X_train)\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "rmse_train = np.sqrt(mse_train)\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "\n",
        "# Display results\n",
        "print(\"Linear Regression Model on Training Set:\")\n",
        "print(\"RMSE:\", rmse_train)\n",
        "print(\"R-squared:\", r2_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de3ljKxv10E1"
      },
      "source": [
        "Polynomial Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q97Qtur13TY"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import numpy as np\n",
        "# Assuming your data is in a DataFrame called 'cleaned_data'\n",
        "# Update the column names based on your specific dataset\n",
        "columns_to_remove = ['Rating Overall', 'prev_rating_ave_pastYear', 'prev_Rating Overall', 'num_5_star_Rev_pastYear', 'prev_num_5_star_Rev_pastYear', 'rating_ave_pastYear']\n",
        "X = sampled_data.drop(columns=columns_to_remove, errors='ignore')  # Independent variables\n",
        "y = sampled_data['Rating Overall']  # Dependent variable\n",
        "\n",
        "# Identify numeric and categorical features\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
        "categorical_features = X.select_dtypes(include=[np.object]).columns\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a preprocessing pipeline\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Append the polynomial features and linear regression model to the preprocessor\n",
        "model_poly_regression = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('poly_features', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Fit the model\n",
        "model_poly_regression.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on the training set\n",
        "y_pred_train_poly = model_poly_regression.predict(X_train)\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "mse_train_poly = mean_squared_error(y_train, y_pred_train_poly)\n",
        "rmse_train_poly = np.sqrt(mse_train_poly)\n",
        "r2_train_poly = r2_score(y_train, y_pred_train_poly)\n",
        "\n",
        "# Display results\n",
        "print(\"Polynomial Regression Model on Training Set:\")\n",
        "print(\"RMSE:\", rmse_train_poly)\n",
        "print(\"R-squared:\", r2_train_poly)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eC7Qs0i0oSz"
      },
      "outputs": [],
      "source": [
        "print(X.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGxDnlpjpPhy"
      },
      "source": [
        "Stepwise Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMIz7GsrbNCi"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Assuming your data is in a DataFrame called 'cleaned_data'\n",
        "\n",
        "\n",
        "columns_to_remove = ['Rating Overall', 'prev_rating_ave_pastYear', 'prev_Rating Overall', 'num_5_star_Rev_pastYear', 'prev_num_5_star_Rev_pastYear', 'rating_ave_pastYear']\n",
        "X = sampled_data.drop(columns=columns_to_remove, errors='ignore')  # Independent variables\n",
        "y = sampled_data['Rating Overall']  # Dependent variable\n",
        "\n",
        "# Identify numeric and categorical features\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
        "categorical_features = X.select_dtypes(include=[np.object]).columns\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a preprocessing pipeline\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Append the polynomial features and linear regression model to the preprocessor\n",
        "model_poly_regression = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('poly_features', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Fit the model\n",
        "model_poly_regression.fit(X_train, y_train)\n",
        "\n",
        "# Perform stepwise regression using statsmodels\n",
        "X_train_with_const = sm.add_constant(X_train)\n",
        "stats_model = sm.OLS(y_train, X_train_with_const).fit()\n",
        "\n",
        "# Perform feature selection using p-values\n",
        "selected_features = stats_model.pvalues[stats_model.pvalues < 0.05].index\n",
        "\n",
        "# Update the model with selected features\n",
        "model_stepwise = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('poly_features', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "model_stepwise.fit(X_train[selected_features], y_train)\n",
        "\n",
        "# Predictions on the training set\n",
        "y_pred_train_stepwise = model_stepwise.predict(X_train[selected_features])\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "mse_train_stepwise = mean_squared_error(y_train, y_pred_train_stepwise)\n",
        "rmse_train_stepwise = np.sqrt(mse_train_stepwise)\n",
        "r2_train_stepwise = r2_score(y_train, y_pred_train_stepwise)\n",
        "\n",
        "# Display results\n",
        "print(\"Stepwise Polynomial Regression Model on Training Set:\")\n",
        "print(\"RMSE:\", rmse_train_stepwise)\n",
        "print(\"R-squared:\", r2_train_stepwise)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R5BI9NhqeI1"
      },
      "source": [
        "linear reg with interaction terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z6_TZW8Yylt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Add a constant term to the features for statsmodels\n",
        "X_train_with_const = sm.add_constant(X_train)\n",
        "\n",
        "# Fit the model using statsmodels\n",
        "stats_model = sm.OLS(y_train, X_train_with_const).fit()\n",
        "\n",
        "# Display detailed summary\n",
        "print(stats_model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoJvTSzkve0k",
        "outputId": "a0a3e63d-9a30-4ea0-93b4-5f5798fd5ce0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-380ef9a72c71>:21: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  categorical_features = X.select_dtypes(include=[np.object]).columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lasso Regression:\n",
            "RMSE: 148.45163612951117\n",
            "R-squared: 0.1379496704411699\n"
          ]
        }
      ],
      "source": [
        "# Lasso Regression\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'sampled_data' is your DataFrame\n",
        "\n",
        "# Define the columns to remove\n",
        "columns_to_remove = ['Rating Overall', 'prev_rating_ave_pastYear', 'prev_Rating Overall', 'num_5_star_Rev_pastYear', 'prev_num_5_star_Rev_pastYear', 'rating_ave_pastYear']\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = sampled_data.drop(columns=columns_to_remove, errors='ignore')\n",
        "y = sampled_data['Rating Overall']\n",
        "\n",
        "# Identify numeric and categorical features\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
        "categorical_features = X.select_dtypes(include=[np.object]).columns\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Append the Lasso regression model to the preprocessor\n",
        "lasso_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                               ('regressor', Lasso(alpha=0.1))])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the Lasso model\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_lasso = lasso_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Lasso model\n",
        "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
        "\n",
        "# Display results\n",
        "print(\"Lasso Regression:\")\n",
        "print(\"RMSE:\", mse_lasso)\n",
        "print(\"R-squared:\", r2_lasso)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn9a1bXqx68Y",
        "outputId": "a71b4191-e6fb-4c35-8802-b2f80f06dac3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-747c01c9b941>:21: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  categorical_features = X.select_dtypes(include=[np.object]).columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest:\n",
            "RMSE: 65.6346605657267\n",
            "R-squared: 0.6815354205720892\n",
            "\n",
            "Gradient Boosting:\n",
            "RMSE: 55.87491192404369\n",
            "R-squared: 0.728890495157768\n"
          ]
        }
      ],
      "source": [
        "#ensemble: rf & gb\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "# Assuming 'sampled_data' is your DataFrame\n",
        "\n",
        "# Define the columns to remove\n",
        "columns_to_remove = ['Rating Overall', 'prev_rating_ave_pastYear', 'prev_Rating Overall', 'num_5_star_Rev_pastYear', 'prev_num_5_star_Rev_pastYear', 'rating_ave_pastYear']\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = sampled_data.drop(columns=columns_to_remove, errors='ignore')\n",
        "y = sampled_data['Rating Overall']\n",
        "\n",
        "# Identify numeric and categorical features\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
        "categorical_features = X.select_dtypes(include=[np.object]).columns\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Random Forest\n",
        "rf_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))])\n",
        "\n",
        "# Gradient Boosting\n",
        "gb_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('regressor', GradientBoostingRegressor(n_estimators=100, random_state=42))])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Fit the Random Forest model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Fit the Gradient Boosting model\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "# Evaluate Random Forest\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "# Evaluate Gradient Boosting\n",
        "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
        "r2_gb = r2_score(y_test, y_pred_gb)\n",
        "\n",
        "# Display results\n",
        "print(\"Random Forest:\")\n",
        "print(\"RMSE:\", mse_rf)\n",
        "print(\"R-squared:\", r2_rf)\n",
        "\n",
        "print(\"\\nGradient Boosting:\")\n",
        "print(\"RMSE:\", mse_gb)\n",
        "print(\"R-squared:\", r2_gb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr5OXJDfhbsi",
        "outputId": "6d17ee6e-29c8-4393-8fdb-49fef0806db5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-b5474ecdfaf3>:20: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  categorical_features = X.select_dtypes(include=[np.object]).columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest:\n",
            "RMSE: 67.51365154677585\n",
            "R-squared: 0.663079116822715\n",
            "\n",
            "Gradient Boosting:\n",
            "RMSE: 54.56725467799338\n",
            "R-squared: 0.7276869608225576\n"
          ]
        }
      ],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'sampled_data' is your DataFrame\n",
        "\n",
        "# Define the columns to remove\n",
        "columns_to_remove = ['Rating Overall', 'prev_rating_ave_pastYear', 'prev_Rating Overall', 'num_5_star_Rev_pastYear', 'prev_num_5_star_Rev_pastYear', 'rating_ave_pastYear']\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = sampled_data.drop(columns=columns_to_remove, errors='ignore')\n",
        "y = sampled_data['Rating Overall']\n",
        "\n",
        "# Identify numeric and categorical features\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
        "categorical_features = X.select_dtypes(include=[np.object]).columns\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Random Forest\n",
        "rf_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))])\n",
        "\n",
        "# Gradient Boosting\n",
        "gb_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('regressor', GradientBoostingRegressor(n_estimators=100, random_state=42))])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Fit the Random Forest model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Fit the Gradient Boosting model\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "# Evaluate Random Forest\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "# Evaluate Gradient Boosting\n",
        "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
        "r2_gb = r2_score(y_test, y_pred_gb)\n",
        "\n",
        "# Display results\n",
        "print(\"Random Forest:\")\n",
        "print(\"RMSE:\", mse_rf)\n",
        "print(\"R-squared:\", r2_rf)\n",
        "\n",
        "print(\"\\nGradient Boosting:\")\n",
        "print(\"RMSE:\", mse_gb)\n",
        "print(\"R-squared:\", r2_gb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7rUekUd6EfG",
        "outputId": "a960f223-f5d2-4c64-aafd-821795258304"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validated RMSE Scores: [-7.88623124 -8.03323139 -8.18094752 -7.97439956 -8.30585438]\n",
            "Mean RMSE Score: -8.076132816642858\n",
            "\n",
            "Cross-Validated R-squared Scores: [0.70510425 0.69579152 0.6610201  0.70225604 0.68967316]\n",
            "Mean R-squared Score: 0.6907690140718816\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Assuming 'sampled_data' is your DataFrame\n",
        "# Choose different features for the model\n",
        "# features = ['Airbnb Host ID', 'Airbnb Property ID', 'City_x', 'superhost_period_all',\n",
        "#             'host_is_superhost_in_period', 'prev_host_is_superhost', 'superhost_change',\n",
        "#             'superhost_change_lose_superhost', 'superhost_change_gain_superhost', 'numReviews_pastYear',\n",
        "#             'numCancel_pastYear', 'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "#             'prev_numReviews_pastYear', 'prev_numCancel_pastYear',\n",
        "#             'prev_num_5_star_Rev_pastYear', 'prev_prop_5_StarReviews_pastYear',\n",
        "#             'numReservedDays_pastYear', 'numReserv_pastYear', 'prev_numReservedDays_pastYear',\n",
        "#             'prev_numReserv_pastYear', 'available_days', 'available_days_aveListedPrice',\n",
        "#             'booked_days', 'booked_days_avePrice', 'prev_available_days', 'prev_available_days_aveListedPrice',\n",
        "#             'prev_booked_days', 'prev_booked_days_avePrice', 'Property Type', 'Listing Type',\n",
        "#             'Created Date', 'Zipcode', 'Bedrooms', 'Bathrooms', 'Neighborhood', 'Max Guests',\n",
        "#             'Cleaning Fee (USD)', 'Minimum Stay', 'Number of Photos', 'Latitude', 'Longitude',\n",
        "#             'Pets Allowed', 'Instantbook Enabled', 'prev_Instantbook Enabled', 'Nightly Rate',\n",
        "#             'prev_Nightly Rate', 'Number of Reviews', 'prev_Number of Reviews']\n",
        "\n",
        "features=['Listing Type','numReviews_pastYear','booked_days','num_5_star_Rev_pastYear','prop_5_StarReviews_pastYear','Minimum Stay','Max Guests','Zipcode','Pets Allowed','available_days','Number of Reviews','Number of Photos','Instantbook Enabled','Bathrooms','Bedrooms','Nightly Rate','Cleaning Fee (USD)']\n",
        "X = cleaned_data[features]\n",
        "y = cleaned_data['Rating Overall']  # Dependent variable\n",
        "\n",
        "# Preprocessing\n",
        "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)])\n",
        "\n",
        "# Ensemble of Regressors\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "lasso = Lasso(alpha=0.1)\n",
        "gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "ensemble_model = VotingRegressor(estimators=[\n",
        "    ('rf', rf),\n",
        "    ('lasso', lasso),\n",
        "    ('gb', gb)],\n",
        "    )\n",
        "\n",
        "# Pipeline without SMOTE\n",
        "pipeline = ImbPipeline(steps=[('preprocessor', preprocessor),\n",
        "                              ('regressor', ensemble_model)])\n",
        "\n",
        "# K-Fold for cross-validation\n",
        "k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Cross-validation scores\n",
        "cv_scores_rmse = cross_val_score(pipeline, X, y, cv=k_fold, scoring='neg_root_mean_squared_error')\n",
        "cv_scores_r2 = cross_val_score(pipeline, X, y, cv=k_fold, scoring='r2')\n",
        "\n",
        "print(f'Cross-Validated RMSE Scores: {cv_scores_rmse}')\n",
        "print(f'Mean RMSE Score: {cv_scores_rmse.mean()}')\n",
        "\n",
        "print(f'\\nCross-Validated R-squared Scores: {cv_scores_r2}')\n",
        "print(f'Mean R-squared Score: {cv_scores_r2.mean()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CFqq0sOi5T_",
        "outputId": "b464392c-b11b-499c-94dd-7a2d2dbc59e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validated RMSE Scores: [-4.56614297 -6.02463959 -5.39194968 -4.80253109 -4.64941117]\n",
            "Mean RMSE Score: -5.086934901918474\n",
            "\n",
            "Cross-Validated R-squared Scores: [0.87892718 0.85121616 0.86950765 0.89430933 0.89258482]\n",
            "Mean R-squared Score: 0.8773090282950754\n"
          ]
        }
      ],
      "source": [
        "#features=['host_is_superhost_in_period','prev_host_is_superhost_in_period','Listing Type','numReviews_pastYear','booked_days','num_5_star_Rev_pastYear','prop_5_StarReviews_pastYear','Minimum Stay','Max Guests','Zipcode','Pets Allowed','available_days','prev_booked_days','Number of Reviews','Number of Photos','Instantbook Enabled','Bathrooms','prev_available_days_aveListedPrice','Nightly Rate','Cleaning Fee (USD)']\n",
        "features=['prev_Rating Overall', 'Number of Reviews', 'rating_ave_pastYear',\n",
        " 'prev_Number of Reviews', 'booked_days_avePrice', 'tract_housing_units',\n",
        " 'prop_5_StarReviews_pastYear', 'prev_revenue', 'prev_Nightly Rate',\n",
        " 'numCancel_pastYear',  'tract_prev_superhosts_ratio', 'revenue',\n",
        " 'prev_rating_ave_pastYear', 'prev_prop_5_StarReviews_pastYear',\n",
        " 'available_days_aveListedPrice', 'numReserv_pastYear',\n",
        " 'tract_revenue_share', 'prev_numReservedDays_pastYear',\n",
        " 'tract_total_pop', 'tract_count_obs', 'tract_price_variance',\n",
        " 'tract_superhosts_ratio', 'Listing Type', 'numReviews_pastYear', 'booked_days',\n",
        " 'num_5_star_Rev_pastYear', 'Minimum Stay', 'Max Guests', 'Zipcode',\n",
        " 'Pets Allowed', 'available_days', 'Number of Photos',\n",
        " 'Instantbook Enabled', 'Bathrooms', 'Bedrooms', 'Nightly Rate', 'Cleaning Fee (USD)']\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Assuming 'sampled_data' is your DataFrame\n",
        "# Choose different features for the model\n",
        "# features = ['Airbnb Host ID', 'Airbnb Property ID', 'City_x', 'superhost_period_all',\n",
        "#             'host_is_superhost_in_period', 'prev_host_is_superhost', 'superhost_change',\n",
        "#             'superhost_change_lose_superhost', 'superhost_change_gain_superhost', 'numReviews_pastYear',\n",
        "#             'numCancel_pastYear', 'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "#             'prev_numReviews_pastYear', 'prev_numCancel_pastYear',\n",
        "#             'prev_num_5_star_Rev_pastYear', 'prev_prop_5_StarReviews_pastYear',\n",
        "#             'numReservedDays_pastYear', 'numReserv_pastYear', 'prev_numReservedDays_pastYear',\n",
        "#             'prev_numReserv_pastYear', 'available_days', 'available_days_aveListedPrice',\n",
        "#             'booked_days', 'booked_days_avePrice', 'prev_available_days', 'prev_available_days_aveListedPrice',\n",
        "#             'prev_booked_days', 'prev_booked_days_avePrice', 'Property Type', 'Listing Type',\n",
        "#             'Created Date', 'Zipcode', 'Bedrooms', 'Bathrooms', 'Neighborhood', 'Max Guests',\n",
        "#             'Cleaning Fee (USD)', 'Minimum Stay', 'Number of Photos', 'Latitude', 'Longitude',\n",
        "#             'Pets Allowed', 'Instantbook Enabled', 'prev_Instantbook Enabled', 'Nightly Rate',\n",
        "#             'prev_Nightly Rate', 'Number of Reviews', 'prev_Number of Reviews']\n",
        "\n",
        "#features=['Listing Type','numReviews_pastYear','booked_days','num_5_star_Rev_pastYear','prop_5_StarReviews_pastYear','Minimum Stay','Max Guests','Zipcode','Pets Allowed','available_days','Number of Reviews','Number of Photos','Instantbook Enabled','Bathrooms','Bedrooms','Nightly Rate','Cleaning Fee (USD)']\n",
        "X = sampled_data[features]\n",
        "y = sampled_data['Rating Overall']  # Dependent variable\n",
        "\n",
        "# Preprocessing\n",
        "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)])\n",
        "\n",
        "# Ensemble of Regressors\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "lasso = Lasso(alpha=0.1)\n",
        "gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "ensemble_model = VotingRegressor(estimators=[\n",
        "    ('rf', rf),\n",
        "    ('lasso', lasso),\n",
        "    ('gb', gb)],\n",
        "    )\n",
        "\n",
        "# Pipeline without SMOTE\n",
        "pipeline = ImbPipeline(steps=[('preprocessor', preprocessor),\n",
        "                              ('regressor', ensemble_model)])\n",
        "\n",
        "# K-Fold for cross-validation\n",
        "k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Cross-validation scores\n",
        "cv_scores_rmse = cross_val_score(pipeline, X, y, cv=k_fold, scoring='neg_root_mean_squared_error')\n",
        "cv_scores_r2 = cross_val_score(pipeline, X, y, cv=k_fold, scoring='r2')\n",
        "\n",
        "print(f'Cross-Validated RMSE Scores: {cv_scores_rmse}')\n",
        "print(f'Mean RMSE Score: {cv_scores_rmse.mean()}')\n",
        "\n",
        "print(f'\\nCross-Validated R-squared Scores: {cv_scores_r2}')\n",
        "print(f'Mean R-squared Score: {cv_scores_r2.mean()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB1CYwe63-lT"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avLkmkCrwWbS",
        "outputId": "193a46e9-f875-440e-880a-e369ad312349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rf Feature Importances:\n",
            "[8.49883723e-01 2.99304915e-02 4.47435742e-03 6.11894952e-03\n",
            " 8.11829002e-03 1.62051937e-03 3.21659361e-03 9.96650522e-03\n",
            " 2.18507391e-03 1.74963763e-03 5.31457142e-03 2.96068866e-03\n",
            " 5.11297431e-03 1.83711303e-03 2.15002811e-03 7.13017454e-03\n",
            " 4.14978366e-03 2.69013781e-03 2.74195718e-03 1.08314289e-03\n",
            " 3.86817177e-03 3.27814756e-03 4.16108822e-03 2.72977771e-03\n",
            " 3.00003759e-03 2.66727909e-03 2.19020891e-03 2.52123326e-03\n",
            " 1.54201215e-03 4.19339937e-03 4.72140469e-03 2.78221064e-04\n",
            " 5.89990425e-04 1.57884779e-03 4.14372192e-03 5.86178391e-03\n",
            " 9.55932456e-05 1.60581786e-05 1.14052977e-04 1.42570450e-05]\n",
            "\n",
            "\n",
            "lasso Coefficients:\n",
            "[ 1.31903437e+01  7.99522454e-02  6.59892446e-02  0.00000000e+00\n",
            " -1.20377463e-01 -0.00000000e+00  6.50529182e-02  0.00000000e+00\n",
            "  0.00000000e+00 -2.64788951e-02  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  7.43016341e-02 -0.00000000e+00\n",
            " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -1.14542556e-03\n",
            " -6.75072032e-02 -0.00000000e+00  1.07546871e-02 -0.00000000e+00\n",
            "  3.98584269e-02 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            " -0.00000000e+00 -8.98267138e-02  7.13949624e-02 -0.00000000e+00\n",
            "  2.37832277e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00]\n",
            "\n",
            "\n",
            "gb Feature Importances:\n",
            "[9.24491155e-01 3.39044450e-02 1.18753195e-03 8.47096479e-03\n",
            " 1.94886337e-03 1.80249741e-04 2.00803361e-03 1.60808492e-03\n",
            " 3.58057419e-04 1.16852828e-04 5.36205747e-03 4.91705614e-04\n",
            " 1.07149576e-03 5.25890375e-04 5.10474407e-04 3.55078092e-03\n",
            " 1.11198557e-03 3.53432055e-04 1.21135017e-04 3.19326926e-05\n",
            " 3.02783867e-03 9.57221678e-05 8.99725447e-04 4.89804911e-04\n",
            " 9.79869494e-05 3.41249177e-04 2.33671801e-04 7.30973408e-04\n",
            " 4.41083865e-04 5.58576307e-04 3.58122188e-03 2.02417770e-04\n",
            " 7.24552956e-05 2.54370219e-04 2.98136755e-04 1.26963732e-03\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Fit the model on the entire dataset\n",
        "pipeline.fit(X, y)\n",
        "\n",
        "# Access the individual estimators from the ensemble\n",
        "estimators = pipeline.named_steps['regressor'].estimators_\n",
        "\n",
        "# Access the feature importances for each estimator\n",
        "for name, estimator in zip(['rf', 'lasso', 'gb', 'poly_reg'], estimators):\n",
        "    if hasattr(estimator, 'feature_importances_'):\n",
        "        feature_importances = estimator.feature_importances_\n",
        "        print(f'{name} Feature Importances:')\n",
        "        print(feature_importances)\n",
        "    elif hasattr(estimator, 'coef_'):\n",
        "        coef = estimator.coef_\n",
        "        print(f'{name} Coefficients:')\n",
        "        print(coef)\n",
        "    else:\n",
        "        print(f'{name} does not provide feature importances or coefficients.')\n",
        "    print('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzWGosfBkgZZ"
      },
      "source": [
        "ensemble (w/o biz context): Cross-Validated RMSE Scores: [-4.55192707 -6.12011155 -5.39353755 -4.77253577 -4.65419773]\n",
        "Mean RMSE Score: -5.098461933598681\n",
        "\n",
        "Cross-Validated R-squared Scores: [0.87967989 0.84646327 0.86943078 0.89562544 0.89236353]\n",
        "Mean R-squared Score: 0.8767125824637135"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "AZH8dqLyd_Yu",
        "outputId": "6f138ae0-9cf2-4be5-ac00-6e1660f82086"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f9db022ae580>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Split the data into training and validation sets (70:30 ratio)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create a new ensemble model for better clarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and validation sets (70:30 ratio)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a new ensemble model for better clarity\n",
        "ensemble_model_split = VotingRegressor(estimators=[\n",
        "    ('rf', rf),\n",
        "    ('lasso', lasso),\n",
        "    ('gb', gb)],\n",
        ")\n",
        "\n",
        "# Create pipelines for training and validation\n",
        "pipeline_train = ImbPipeline(steps=[('preprocessor', preprocessor),\n",
        "                                     ('regressor', ensemble_model_split)])\n",
        "\n",
        "pipeline_valid = ImbPipeline(steps=[('preprocessor', preprocessor),\n",
        "                                     ('regressor', ensemble_model_split)])\n",
        "\n",
        "# Fit the model on the training data\n",
        "pipeline_train.fit(X_train, y_train)\n",
        "\n",
        "# Predict on training and validation sets\n",
        "y_train_pred = pipeline_train.predict(X_train)\n",
        "y_valid_pred = pipeline_train.predict(X_valid)\n",
        "\n",
        "# Calculate R-squared scores\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "r2_valid = r2_score(y_valid, y_valid_pred)\n",
        "\n",
        "print(f'Training R-squared: {r2_train}')\n",
        "print(f'Validation R-squared: {r2_valid}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "a9OzJpRWoLK6",
        "outputId": "e1484ea0-2702-47ef-d91b-ddb1924be5d3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-615ab6e820f3>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Split the data into training and validation sets (70:30 ratio)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#linear_reg = LinearRegression()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#poly_reg = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Split the data into training and validation sets (70:30 ratio)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "#linear_reg = LinearRegression()\n",
        "#poly_reg = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "\n",
        "ensemble_model = VotingRegressor(estimators=[\n",
        "    ('rf', rf),\n",
        "    ('lasso', lasso),\n",
        "    ('gb', gb)],\n",
        ")\n",
        "\n",
        "# Create pipelines for training and validation\n",
        "pipeline_train = ImbPipeline(steps=[('preprocessor', preprocessor),\n",
        "                                     ('regressor', ensemble_model_split)])\n",
        "\n",
        "pipeline_valid = ImbPipeline(steps=[('preprocessor', preprocessor),\n",
        "                                     ('regressor', ensemble_model_split)])\n",
        "\n",
        "# Fit the model on the training data\n",
        "pipeline_train.fit(X_train, y_train)\n",
        "\n",
        "# Predict on training and validation sets\n",
        "y_train_pred = pipeline_train.predict(X_train)\n",
        "y_valid_pred = pipeline_train.predict(X_valid)\n",
        "\n",
        "# Calculate R-squared scores\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "r2_valid = r2_score(y_valid, y_valid_pred)\n",
        "\n",
        "print(f'Training R-squared: {r2_train}')\n",
        "print(f'Validation R-squared: {r2_valid}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDP_o7igBA1X"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/your_dataset.csv')  # Update with your dataset path\n",
        "\n",
        "# Define the target variable and features\n",
        "target = 'occupancy_rate'  # Update with your actual occupancy rate column name\n",
        "features = ['host_is_superhost_in_period', 'numReviews_pastYear', 'booked_days',\n",
        "            'rating_ave_pastYear', 'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "            'prev_rating_ave_pastYear', 'prev_prop_5_StarReviews_pastYear', 'Minimum Stay',\n",
        "            'Max Guests', 'Zipcode', 'Pets Allowed', 'available_days', 'Nightly Rate',\n",
        "            'Cleaning Fee (USD)', 'Bedrooms', 'Bathrooms', ]\n",
        "\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Preprocessing\n",
        "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)])\n",
        "\n",
        "# Ensemble of Regressors\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "ensemble_model = VotingRegressor(estimators=[\n",
        "    ('rf', rf),\n",
        "    ('gb', gb)])\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('regressor', ensemble_model)])\n",
        "\n",
        "# K-Fold for cross-validation\n",
        "k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Cross-validation scores\n",
        "cv_scores_rmse = cross_val_score(pipeline, X, y, cv=k_fold, scoring='neg_root_mean_squared_error')\n",
        "cv_scores_r2 = cross_val_score(pipeline, X, y, cv=k_fold, scoring='r2')\n",
        "\n",
        "print(f'Cross-Validated RMSE Scores: {cv_scores_rmse}')\n",
        "print(f'Mean RMSE Score: {cv_scores_rmse.mean()}')\n",
        "print(f'\\nCross-Validated R-squared Scores: {cv_scores_r2}')\n",
        "print(f'Mean R-squared Score: {cv_scores_r2.mean()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "l0fcBaiqJS9y",
        "outputId": "5f7e1f63-c487-4bce-e778-e75b697bafc4"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-94a33f87c1fe>\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Associate coefficients with feature names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mfeature_importances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Feature'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Coefficient'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlinear_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Sort the features by the absolute value of their coefficients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures, MaxAbsScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Function to calculate MAPE\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    non_zero_mask = y_true != 0\n",
        "    return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/AirbnbHouston_Preprocessed_dataset 2.csv')  # Adjust path as needed\n",
        "\n",
        "# Define the target variable and features\n",
        "target = 'occupancy_rate'\n",
        "features = ['host_is_superhost_in_period', 'numReviews_pastYear', 'booked_days',\n",
        "            'rating_ave_pastYear', 'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "            'prev_rating_ave_pastYear', 'prev_prop_5_StarReviews_pastYear', 'Minimum Stay',\n",
        "            'Max Guests', 'Zipcode', 'Pets Allowed', 'available_days', 'Nightly Rate',\n",
        "            'Cleaning Fee (USD)', 'Bedrooms', 'Bathrooms', 'Rating Overall', 'superhost_change_lose_superhost','superhost_change_gain_superhost']\n",
        "\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Splitting the dataset into training and validation sets (60:40)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Preprocessing\n",
        "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', MaxAbsScaler())])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)])\n",
        "\n",
        "# Fit and transform training data\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_val_processed = preprocessor.transform(X_val)\n",
        "\n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "X_train_poly = poly_features.fit_transform(X_train_processed)\n",
        "X_val_poly = poly_features.transform(X_val_processed)\n",
        "\n",
        "# Linear Regression Model\n",
        "linear_reg = LinearRegression()\n",
        "linear_reg.fit(X_train_poly, y_train)\n",
        "\n",
        "# Predict and evaluate on training and validation data\n",
        "y_train_pred = linear_reg.predict(X_train_poly)\n",
        "y_val_pred = linear_reg.predict(X_val_poly)\n",
        "\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "val_r2 = r2_score(y_val, y_val_pred)\n",
        "\n",
        "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
        "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
        "\n",
        "# Associate coefficients with feature names\n",
        "feature_importances = pd.DataFrame({'Feature': features, 'Coefficient': linear_reg.coef_})\n",
        "\n",
        "# Sort the features by the absolute value of their coefficients\n",
        "feature_importances['abs_coefficient'] = feature_importances['Coefficient'].abs()\n",
        "feature_importances = feature_importances.sort_values(by='abs_coefficient', ascending=False)\n",
        "\n",
        "# Print the top 10 feature importances\n",
        "print(\"\\nTop 10 Feature Importances:\")\n",
        "print(feature_importances[['Feature', 'Coefficient']].head(10))\n",
        "\n",
        "# Print model summary\n",
        "print(\"Model Summary:\")\n",
        "\n",
        "print(\"Training MSE:\", train_mse)\n",
        "print(\"Training R-squared:\", train_r2)\n",
        "print(\"Training MAPE:\", train_mape, \"%\")\n",
        "print(\"Validation MSE:\", val_mse)\n",
        "print(\"Validation R-squared:\", val_r2)\n",
        "print(\"Validation MAPE:\", val_mape, \"%\")\n",
        "\n",
        "print(\"Intercept:\", linear_reg.intercept_)\n",
        "print(\"Coefficients:\", linear_reg.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "pwctMAc5I6ER",
        "outputId": "bdccb943-e767-49b6-88c9-f3cc34906358"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-10c26b4352e3>\u001b[0m in \u001b[0;36m<cell line: 107>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m# Formatting the DataFrame for better readability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0mmodel_comparison\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_comparison\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Round to 4 decimal places for clarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mmodel_comparison\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_comparison\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation R2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sort by Validation R2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# Display model comparison in table format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6911\u001b[0m             \u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6912\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6914\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1848\u001b[0m             )\n\u001b[1;32m   1849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Validation R2'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures, MaxAbsScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/AirbnbHouston_Preprocessed_dataset 2.csv')  # Adjust path as needed\n",
        "\n",
        "# Define the target variable and features\n",
        "target = 'occupancy_rate'\n",
        "features = ['host_is_superhost_in_period', 'numReviews_pastYear', 'booked_days',\n",
        "            'rating_ave_pastYear', 'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "            'prev_rating_ave_pastYear', 'prev_prop_5_StarReviews_pastYear', 'Minimum Stay',\n",
        "            'Max Guests', 'Zipcode', 'Pets Allowed', 'available_days', 'Nightly Rate',\n",
        "            'Cleaning Fee (USD)', 'Bedrooms', 'Bathrooms', 'Rating Overall', 'superhost_change_lose_superhost', 'superhost_change_gain_superhost']\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Splitting the dataset into training and validation sets (60:40)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Preprocessing\n",
        "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', MaxAbsScaler())])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)])\n",
        "\n",
        "# Initialize models including Polynomial Regression\n",
        "models = {\n",
        "    \"Polynomial Regression\": Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('poly_features', PolynomialFeatures(degree=2)),\n",
        "        ('regressor', LinearRegression())\n",
        "    ]),\n",
        "    \"Linear Regression\": Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', LinearRegression())\n",
        "    ]),\n",
        "    \"Random Forest\": Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', RandomForestRegressor(random_state=42))\n",
        "    ]),\n",
        "    \"Gradient Boosting\": Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', GradientBoostingRegressor(random_state=42))\n",
        "    ]),\n",
        "    \"XGBoost\": Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', XGBRegressor(random_state=42))\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Fit models and calculate R-squared and RMSE for each\n",
        "model_performance = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)  # Fit the model\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "\n",
        "    # Calculate R-squared\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "    val_r2 = r2_score(y_val, y_val_pred)\n",
        "\n",
        "    # Calculate RMSE\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "\n",
        "    # Store performance\n",
        "    model_performance[model_name] = {\n",
        "        'Training R2': train_r2, 'Validation R2': val_r2,\n",
        "        'Training RMSE': train_rmse, 'Validation RMSE': val_rmse\n",
        "    }\n",
        "\n",
        "# Creating a DataFrame for model comparison\n",
        "model_comparison = pd.DataFrame(model_performance).T\n",
        "\n",
        "# Display model comparison\n",
        "print(\"Model Comparison (R-squared and RMSE):\")\n",
        "print(model_comparison)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDfW8Jf4P0VL",
        "outputId": "0e20a009-f59e-488a-c5ae-d6e7de86ecbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top Polynomial Features Importance:\n",
            "x1 x10: 1075.6034065320425\n",
            "x10^2: 2165.6752091472185\n",
            "x10: 2173.042785818199\n",
            "x18^2: 324349499487630.1\n",
            "x0 x18: 1.2818210520389102e+17\n",
            "x18: 1.414023236919342e+17\n",
            "x17^2: 4.9093730309821415e+19\n",
            "x17: 4.909373038756536e+19\n",
            "x0^2: 6.4127339977866234e+22\n",
            "x0: 6.412733998203141e+22\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures, MaxAbsScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/AirbnbHouston_Preprocessed_dataset 2.csv')  # Adjust path as needed\n",
        "\n",
        "# Define the target variable and features\n",
        "target = 'occupancy_rate'\n",
        "features = ['host_is_superhost_in_period', 'numReviews_pastYear', 'booked_days',\n",
        "            'rating_ave_pastYear', 'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "            'prev_rating_ave_pastYear', 'prev_prop_5_StarReviews_pastYear', 'Minimum Stay',\n",
        "            'Max Guests', 'Zipcode', 'Pets Allowed', 'available_days', 'Nightly Rate',\n",
        "            'Cleaning Fee (USD)', 'Bedrooms', 'Bathrooms', 'Rating Overall', 'superhost_change_lose_superhost', 'superhost_change_gain_superhost']\n",
        "\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Splitting the dataset into training and validation sets (60:40)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Preprocessing\n",
        "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', MaxAbsScaler())])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)])\n",
        "\n",
        "# Fit and transform training data\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_val_processed = preprocessor.transform(X_val)\n",
        "\n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "X_train_poly = poly_features.fit_transform(X_train_processed)\n",
        "X_val_poly = poly_features.transform(X_val_processed)\n",
        "\n",
        "# Fit the Linear Regression model\n",
        "linear_reg = LinearRegression()\n",
        "linear_reg.fit(X_train_poly, y_train)\n",
        "\n",
        "# Calculate permutation importance\n",
        "perm_importance = permutation_importance(linear_reg, X_val_poly, y_val, n_repeats=30, random_state=42)\n",
        "\n",
        "# Associate importances with feature names\n",
        "# Since the number of polynomial features is typically large, we will show only the top features\n",
        "top_features = np.argsort(perm_importance.importances_mean)[-10:]  # Adjust the number as needed\n",
        "\n",
        "print(\"Top Polynomial Features Importance:\")\n",
        "for i in top_features:\n",
        "    print(f\"{poly_features.get_feature_names_out()[i]}: {perm_importance.importances_mean[i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIuqrfMPUoqv",
        "outputId": "eecc1933-d9c3-450c-d501-aebdc0c12a69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The correlation between occupancy rate and overall ratings is: 0.037557957332161454\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"AirbnbHouston_Preprocessed_dataset 2.csv\"  # Update this to your actual file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Assuming 'occupancy_rate' and 'Rating Overall' are the correct column names\n",
        "# Convert both columns to numeric, just in case they're not\n",
        "data['occupancy_rate'] = pd.to_numeric(data['Rating Overall'], errors='coerce')\n",
        "data['Rating Overall'] = pd.to_numeric(data['revenue'], errors='coerce')\n",
        "\n",
        "# Calculate the correlation\n",
        "correlation = data[['occupancy_rate', 'Rating Overall']].corr().iloc[0,1]\n",
        "\n",
        "print(f\"The correlation between occupancy rate and overall ratings is: {correlation}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMMcnSvYmSE2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create residual plots\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Residual plot for training data\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_train_pred, y_train_pred - y_train, c='blue', marker='o', label='Training Data')\n",
        "plt.title('Residual Plot for Training Data')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.axhline(y=0, color='gray', linestyle='--')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "# Residual plot for validation data\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(y_val_pred, y_val_pred - y_val, c='green', marker='s', label='Validation Data')\n",
        "plt.title('Residual Plot for Validation Data')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.axhline(y=0, color='gray', linestyle='--')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create feature importance plot (if applicable)\n",
        "if hasattr(linear_reg, 'coef_'):\n",
        "    coef_names = poly_features.get_feature_names(input_features=X.columns)\n",
        "    coef_values = linear_reg.coef_\n",
        "\n",
        "    # Sort coefficients by absolute value\n",
        "    sorted_indices = np.argsort(np.abs(coef_values))[::-1]\n",
        "    sorted_coef_names = [coef_names[i] for i in sorted_indices]\n",
        "    sorted_coef_values = coef_values[sorted_indices]\n",
        "\n",
        "    # Plot top N important features\n",
        "    top_n = 10\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x=sorted_coef_values[:top_n], y=sorted_coef_names[:top_n], orient='h')\n",
        "    plt.title('Top {} Important Features'.format(top_n))\n",
        "    plt.xlabel('Coefficient Value')\n",
        "    plt.ylabel('Feature Name')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures, MaxAbsScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        " \n",
        "# Function to calculate MAPE\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    non_zero_mask = y_true != 0\n",
        "    return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
        " \n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/AirbnbHouston_Preprocessed_dataset 2.csv')  # Adjust path as needed\n",
        " \n",
        "# Define the target variable and features\n",
        "target = 'occupancy_rate'\n",
        "features = ['host_is_superhost_in_period', 'numReviews_pastYear', 'booked_days',\n",
        "            'rating_ave_pastYear', 'num_5_star_Rev_pastYear', 'prop_5_StarReviews_pastYear',\n",
        "            'prev_rating_ave_pastYear', 'prev_prop_5_StarReviews_pastYear', 'Minimum Stay',\n",
        "            'Max Guests', 'Zipcode', 'Pets Allowed', 'available_days', 'Nightly Rate',\n",
        "            'Cleaning Fee (USD)', 'Bedrooms', 'Bathrooms', 'Rating Overall', 'superhost_change_lose_superhost','superhost_change_gain_superhost']\n",
        " \n",
        "X = data[features]\n",
        "y = data[target]\n",
        " \n",
        "# Splitting the dataset into training and validation sets (60:40)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        " \n",
        "# Preprocessing\n",
        "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        " \n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', MaxAbsScaler())])\n",
        " \n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        " \n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)])\n",
        " \n",
        "# Fit and transform training data\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_val_processed = preprocessor.transform(X_val)\n",
        " \n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "X_train_poly = poly_features.fit_transform(X_train_processed)\n",
        "X_val_poly = poly_features.transform(X_val_processed)\n",
        " \n",
        "# Linear Regression Model\n",
        "linear_reg = LinearRegression()\n",
        "linear_reg.fit(X_train_poly, y_train)\n",
        " \n",
        "# Predict and evaluate on training and validation data\n",
        "y_train_pred = linear_reg.predict(X_train_poly)\n",
        "y_val_pred = linear_reg.predict(X_val_poly)\n",
        " \n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "val_r2 = r2_score(y_val, y_val_pred)\n",
        " \n",
        "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
        "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
        " \n",
        "# Print model summary\n",
        "print(\"Model Summary:\")\n",
        " \n",
        "print(\"Training MSE:\", train_mse)\n",
        "print(\"Training R-squared:\", train_r2)\n",
        "print(\"Training MAPE:\", train_mape, \"%\")\n",
        "print(\"Validation MSE:\", val_mse)\n",
        "print(\"Validation R-squared:\", val_r2)\n",
        "print(\"Validation MAPE:\", val_mape, \"%\")\n",
        " \n",
        "print(\"Intercept:\", linear_reg.intercept_)\n",
        "print(\"Coefficients:\", linear_reg.coef_)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
